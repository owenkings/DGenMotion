"""
æ‰¹é‡ç”Ÿæˆ10Ké™æ€å§¿æ€
æ”¯æŒå¤šGPUå¹¶è¡Œã€è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„ã€ä»¥æ–‡æœ¬å‘½åæ–‡ä»¶
"""

import os
from os.path import join as pjoin
import torch
import torch.nn.functional as F
from torch.distributions.categorical import Categorical
import numpy as np
import random
from models.AE import AE_models
from models.MARDM import MARDM_models
from models.LengthEstimator import LengthEstimator
from utils.motion_process import recover_from_ric, kit_kinematic_chain, t2m_kinematic_chain
import argparse
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import json
import re
import shutil
from tqdm import tqdm


def sanitize_filename(text, max_length=200):
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºåˆæ³•çš„æ–‡ä»¶å
    """
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    text = re.sub(r'[<>:"/\\|?*]', '', text)
    # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    text = text.replace(' ', '_')
    # ç§»é™¤å¼€å¤´çš„"A_person_"
    if text.startswith('A_person_'):
        text = text[9:]
    # é™åˆ¶é•¿åº¦
    if len(text) > max_length:
        text = text[:max_length]
    return text


def plot_single_pose_3d(joints, kinematic_tree, title="", save_path=None, figsize=(8, 8)):
    """
    ç»˜åˆ¶å•å¸§3Då§¿æ€
    """
    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111, projection='3d')
    
    # è®¾ç½®åæ ‡è½´
    RADIUS = 4
    ax.set_xlim3d([-RADIUS / 2, RADIUS / 2])
    ax.set_ylim3d([0, RADIUS])
    ax.set_zlim3d([-RADIUS / 3., RADIUS * 2 / 3.])
    
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title(title, fontsize=10, pad=10)
    
    # è°ƒæ•´è§†è§’
    ax.view_init(elev=110, azim=-90)
    ax.dist = 7.5
    
    # ç»˜åˆ¶å…³èŠ‚ç‚¹
    ax.scatter(joints[:, 0], joints[:, 1], joints[:, 2], 
               c='red', marker='o', s=50, alpha=0.8)
    
    # å®šä¹‰ä¸åŒéƒ¨ä½çš„é¢œè‰²
    colors = ['red', 'blue', 'black', 'red', 'blue',  
              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',
              'darkred', 'darkred', 'darkred', 'darkred', 'darkred']
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥
    for i, chain in enumerate(kinematic_tree):
        if i < len(colors):
            linewidth = 4.0
        else:
            linewidth = 2.0
        color = colors[i % len(colors)]
        
        for j in range(len(chain) - 1):
            parent_idx = chain[j]
            child_idx = chain[j + 1]
            ax.plot([joints[parent_idx, 0], joints[child_idx, 0]],
                   [joints[parent_idx, 1], joints[child_idx, 1]],
                   [joints[parent_idx, 2], joints[child_idx, 2]],
                   color=color, linewidth=linewidth, alpha=0.8)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    
    plt.close()


def main(args):
    #################################################################################
    #                                      Seed                                     #
    #################################################################################
    torch.backends.cudnn.benchmark = False
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cudnn.allow_tf32 = False
    
    #################################################################################
    #                                       Data                                    #
    #################################################################################
    dim_pose = 64 if args.dataset_name == 'kit' or args.dataset_name =='eval_kit'else 67
    nb_joints = 21 if args.dataset_name == 'kit' or args.dataset_name =='eval_kit' else 22
    data_root = f'{args.dataset_dir}/KIT-ML/' if args.dataset_name == 'kit' or args.dataset_name =='eval_kit' else f'{args.dataset_dir}/HumanML3D/'
    
    if args.dataset_name =="t2m":
        mean = np.load(pjoin(data_root, 'Mean.npy'))
        std = np.load(pjoin(data_root, 'Std.npy'))
    elif args.dataset_name =="kit":
        mean = np.load(pjoin(data_root, 'Mean.npy'))
        std = np.load(pjoin(data_root, 'Std.npy'))
    elif args.dataset_name =="eval_t2m":
        mean =np.load(pjoin('./utils/eval_mean_std/t2m','eval_mean.npy'))
        std =np.load(pjoin('./utils/eval_mean_std/t2m','eval_std.npy'))
    elif args.dataset_name =="eval_kit":
        mean =np.load(pjoin('./utils/eval_mean_std/kit','eval_mean.npy'))
        std =np.load(pjoin('./utils/eval_mean_std/kit','eval_std.npy'))
    
    #################################################################################
    #                                      Models                                   #
    #################################################################################
    model_dir = pjoin(args.checkpoints_dir, args.dataset_name, args.name, 'model')
    
    # ä¸´æ—¶ç»“æœç›®å½•
    temp_dir = args.temp_dir
    os.makedirs(temp_dir, exist_ok=True)
    
    # æœ€ç»ˆç»“æœç›®å½•
    final_dir = args.final_dir
    os.makedirs(final_dir, exist_ok=True)

    print(f"åŠ è½½æ¨¡å‹...")
    ae = AE_models[args.ae_model](input_width=dim_pose)
    ckpt = torch.load(pjoin(args.checkpoints_dir, args.dataset_name, args.ae_name, 'model',
                            'latest.tar' if args.dataset_name == 't2m' else 'net_best_fid.tar'), map_location='cpu')
    model_key = 'ae'
    ae.load_state_dict(ckpt[model_key])

    ema_mardm = MARDM_models[args.model](ae_dim=ae.output_emb_width, cond_mode='text')
    model_dir = pjoin(model_dir, 'latest.tar')
    checkpoint = torch.load(model_dir, map_location='cpu')
    missing_keys2, unexpected_keys2 = ema_mardm.load_state_dict(checkpoint['ema_mardm'], strict=False)
    assert len(unexpected_keys2) == 0
    assert all([k.startswith('clip_model.') for k in missing_keys2])

    length_estimator = LengthEstimator(512, 50)
    ckpt = torch.load(pjoin(args.checkpoints_dir, args.dataset_name, 'length_estimator', 'model', 'finest.tar'),
                      map_location='cpu')
    length_estimator.load_state_dict(ckpt['estimator'])

    device = torch.device(f"cuda:{args.gpu_id}" if torch.cuda.is_available() else "cpu")

    ae = ae.to(device)
    ema_mardm = ema_mardm.to(device)
    length_estimator = length_estimator.to(device)

    ae.eval()
    ema_mardm.eval()
    length_estimator.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    #################################################################################
    #                                     Sampling                                  #
    #################################################################################
    # è¯»å–æ–‡æœ¬æç¤ºè¯
    prompt_list = []
    with open(args.text_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                prompt_list.append(line)
    
    print(f"ğŸ“ å…±è¯»å– {len(prompt_list)} æ¡æ–‡æœ¬æè¿°")
    print(f"ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
    print(f"ğŸ“ æœ€ç»ˆç›®å½•: {final_dir}")
    print(f"ğŸ¯ åºåˆ—é•¿åº¦: {args.sequence_length} å¸§")
    print(f"ğŸ¯ æå–å¸§ç´¢å¼•: {args.frame_index}")
    print(f"=" * 80)
    
    # ä½¿ç”¨å›ºå®šçš„åºåˆ—é•¿åº¦
    token_lens = torch.LongTensor([args.sequence_length // 4] * len(prompt_list))
    token_lens = token_lens.to(device).long()
    m_length = token_lens * 4
    
    kinematic_chain = kit_kinematic_chain if args.dataset_name == 'kit' else t2m_kinematic_chain
    
    # æ‰¹é‡ç”Ÿæˆ
    batch_size = args.batch_size
    num_batches = (len(prompt_list) + batch_size - 1) // batch_size
    
    success_count = 0
    error_count = 0
    
    for batch_idx in tqdm(range(num_batches), desc=f"GPU {args.gpu_id} ç”Ÿæˆè¿›åº¦"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(prompt_list))
        batch_prompts = prompt_list[start_idx:end_idx]
        batch_token_lens = token_lens[start_idx:end_idx]
        batch_m_length = m_length[start_idx:end_idx]
        
        try:
            with torch.no_grad():
                pred_latents = ema_mardm.generate(batch_prompts, batch_token_lens, args.time_steps, args.cfg,
                                                  temperature=args.temperature, hard_pseudo_reorder=args.hard_pseudo_reorder)
                pred_motions = ae.decode(pred_latents)
                pred_motions = pred_motions.detach().cpu().numpy()
                data = pred_motions * std + mean
            
            # å¤„ç†æ¯ä¸ªæ ·æœ¬
            for i, (caption, joint_data) in enumerate(zip(batch_prompts, data)):
                try:
                    # æˆªå–åˆ°æŒ‡å®šé•¿åº¦
                    joint_data = joint_data[:batch_m_length[i]]
                    
                    # è½¬æ¢ä¸ºXYZåæ ‡
                    joint_sequence = recover_from_ric(torch.from_numpy(joint_data).float(), nb_joints).numpy()
                    
                    # æå–æŒ‡å®šå¸§
                    if args.frame_index >= len(joint_sequence):
                        frame_idx = len(joint_sequence) - 1
                    else:
                        frame_idx = args.frame_index
                    
                    single_pose = joint_sequence[frame_idx]  # (22, 3)
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    safe_name = sanitize_filename(caption)
                    
                    # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºç¼“å­˜ï¼‰
                    temp_npy = pjoin(temp_dir, f"{safe_name}_temp.npy")
                    np.save(temp_npy, single_pose)
                    
                    # ä¿å­˜åˆ°æœ€ç»ˆç›®å½•
                    final_npy = pjoin(final_dir, f"{safe_name}.npy")
                    final_json = pjoin(final_dir, f"{safe_name}.json")
                    final_png = pjoin(final_dir, f"{safe_name}.png")
                    
                    # ä¿å­˜NPY
                    np.save(final_npy, single_pose)
                    
                    # ä¿å­˜JSON
                    json_data = {
                        "caption": caption,
                        "frame_index": frame_idx,
                        "num_joints": nb_joints,
                        "joints": single_pose.tolist(),
                        "joint_names": [
                            "pelvis", "left_hip", "right_hip", "spine1", "left_knee", "right_knee",
                            "spine2", "left_ankle", "right_ankle", "spine3", "left_foot",
                            "right_foot", "neck", "left_collar", "right_collar", "head",
                            "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
                            "left_wrist", "right_wrist"
                        ] if nb_joints == 22 else None
                    }
                    with open(final_json, 'w', encoding='utf-8') as f:
                        json.dump(json_data, f, indent=2, ensure_ascii=False)
                    
                    # ä¿å­˜PNG
                    plot_single_pose_3d(single_pose, kinematic_chain, 
                                       title=caption if len(caption) < 50 else caption[:47] + "...",
                                       save_path=final_png)
                    
                    success_count += 1
                    
                except Exception as e:
                    error_count += 1
                    print(f"\nâŒ å¤„ç†å¤±è´¥: {caption[:50]}... é”™è¯¯: {e}")
                    continue
        
        except Exception as e:
            error_count += len(batch_prompts)
            print(f"\nâŒ æ‰¹æ¬¡ {batch_idx} ç”Ÿæˆå¤±è´¥: {e}")
            continue
    
    print("\n" + "=" * 80)
    print(f"âœ… GPU {args.gpu_id} å®Œæˆï¼")
    print(f"   æˆåŠŸ: {success_count} ä¸ª")
    print(f"   å¤±è´¥: {error_count} ä¸ª")
    print(f"   æ€»è®¡: {len(prompt_list)} ä¸ª")
    print(f"ğŸ“ æœ€ç»ˆç»“æœä¿å­˜åœ¨: {final_dir}")
    print("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ‰¹é‡ç”Ÿæˆ10Ké™æ€å§¿æ€')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--name', type=str, default='MARDM_SiT_XL')
    parser.add_argument('--ae_name', type=str, default="AE")
    parser.add_argument('--ae_model', type=str, default='AE_Model')
    parser.add_argument('--model', type=str, default='MARDM-SiT-XL')
    parser.add_argument('--dataset_name', type=str, default='t2m')
    parser.add_argument('--dataset_dir', type=str, default='./datasets')
    parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--seed", type=int, default=3407)
    parser.add_argument("--time_steps", default=18, type=int)
    parser.add_argument("--cfg", default=4.5, type=float)
    parser.add_argument("--temperature", default=1, type=float)
    parser.add_argument('--hard_pseudo_reorder', action="store_true")
    
    # è¾“å…¥è¾“å‡º
    parser.add_argument('--text_path', type=str, required=True, help='æ–‡æœ¬æè¿°æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--temp_dir', type=str, default='./SinglePose_temp', help='ä¸´æ—¶ç›®å½•')
    parser.add_argument('--final_dir', type=str, default='./SinglePose', help='æœ€ç»ˆç»“æœç›®å½•')
    
    # å§¿æ€å‚æ•°
    parser.add_argument("--sequence_length", default=16, type=int)
    parser.add_argument("--frame_index", default=-1, type=int)
    
    # GPUå’Œæ‰¹å¤„ç†
    parser.add_argument("--gpu_id", type=int, default=0, help='ä½¿ç”¨çš„GPU ID')
    parser.add_argument("--batch_size", type=int, default=8, help='æ‰¹å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    # å¤„ç†frame_index
    if args.frame_index == -1:
        args.frame_index = args.sequence_length // 2
    elif args.frame_index == -2:
        args.frame_index = args.sequence_length - 1
    elif args.frame_index < 0:
        args.frame_index = args.sequence_length + args.frame_index
    
    main(args)


