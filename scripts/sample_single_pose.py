"""
å•å¸§3Då§¿æ€ç”Ÿæˆè„šæœ¬
æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé™æ€çš„å•å¸§3Då§¿æ€ï¼ˆè€Œä¸æ˜¯è¿åŠ¨åºåˆ—ï¼‰

ç”¨æ³•ç¤ºä¾‹:
python sample_single_pose.py --text_prompt "A person raising both hands" --dataset_name t2m
"""

import os
from os.path import join as pjoin
import torch
import torch.nn.functional as F
from torch.distributions.categorical import Categorical
import numpy as np
import random
from models.AE import AE_models
from models.MARDM import MARDM_models
from models.LengthEstimator import LengthEstimator
from utils.motion_process import recover_from_ric, kit_kinematic_chain, t2m_kinematic_chain
import argparse
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import json


def plot_single_pose_3d(joints, kinematic_tree, title="", save_path=None, figsize=(8, 8)):
    """
    ç»˜åˆ¶å•å¸§3Då§¿æ€
    
    Args:
        joints: (22, 3) å•å¸§å…³èŠ‚ç‚¹åæ ‡
        kinematic_tree: éª¨æ¶è¿æ¥å…³ç³»
        title: å›¾ç‰‡æ ‡é¢˜
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111, projection='3d')
    
    # è®¾ç½®åæ ‡è½´
    RADIUS = 4
    ax.set_xlim3d([-RADIUS / 2, RADIUS / 2])
    ax.set_ylim3d([0, RADIUS])
    ax.set_zlim3d([-RADIUS / 3., RADIUS * 2 / 3.])
    
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title(title, fontsize=14, pad=20)
    
    # è°ƒæ•´è§†è§’
    ax.view_init(elev=110, azim=-90)
    ax.dist = 7.5
    
    # ç»˜åˆ¶å…³èŠ‚ç‚¹
    ax.scatter(joints[:, 0], joints[:, 1], joints[:, 2], 
               c='red', marker='o', s=50, alpha=0.8)
    
    # å®šä¹‰ä¸åŒéƒ¨ä½çš„é¢œè‰²
    colors = ['red', 'blue', 'black', 'red', 'blue',  
              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',
              'darkred', 'darkred', 'darkred', 'darkred', 'darkred']
    
    # ç»˜åˆ¶éª¨æ¶è¿æ¥
    for i, chain in enumerate(kinematic_tree):
        if i < len(colors):
            linewidth = 4.0
        else:
            linewidth = 2.0
        color = colors[i % len(colors)]
        
        for j in range(len(chain) - 1):
            parent_idx = chain[j]
            child_idx = chain[j + 1]
            ax.plot([joints[parent_idx, 0], joints[child_idx, 0]],
                   [joints[parent_idx, 1], joints[child_idx, 1]],
                   [joints[parent_idx, 2], joints[child_idx, 2]],
                   color=color, linewidth=linewidth, alpha=0.8)
    
    # æ·»åŠ å…³èŠ‚ç‚¹æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
    # for idx in range(len(joints)):
    #     ax.text(joints[idx, 0], joints[idx, 1], joints[idx, 2], 
    #             str(idx), fontsize=8, color='green')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… å·²ä¿å­˜å›¾ç‰‡: {save_path}")
    else:
        plt.show()
    
    plt.close()


def main(args):
    #################################################################################
    #                                      Seed                                     #
    #################################################################################
    torch.backends.cudnn.benchmark = False
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cudnn.allow_tf32 = False
    
    #################################################################################
    #                                       Data                                    #
    #################################################################################
    dim_pose = 64 if args.dataset_name == 'kit' or args.dataset_name =='eval_kit'else 67
    nb_joints = 21 if args.dataset_name == 'kit' or args.dataset_name =='eval_kit' else 22
    data_root = f'{args.dataset_dir}/KIT-ML/' if args.dataset_name == 'kit' or args.dataset_name =='eval_kit' else f'{args.dataset_dir}/HumanML3D/'
    
    if args.dataset_name =="t2m":
        mean = np.load(pjoin(data_root, 'Mean.npy'))
        std = np.load(pjoin(data_root, 'Std.npy'))
    elif args.dataset_name =="kit":
        mean = np.load(pjoin(data_root, 'Mean.npy'))
        std = np.load(pjoin(data_root, 'Std.npy'))
    elif args.dataset_name =="eval_t2m":
        mean =np.load(pjoin('./utils/eval_mean_std/t2m','eval_mean.npy'))
        std =np.load(pjoin('./utils/eval_mean_std/t2m','eval_std.npy'))
    elif args.dataset_name =="eval_kit":
        mean =np.load(pjoin('./utils/eval_mean_std/kit','eval_mean.npy'))
        std =np.load(pjoin('./utils/eval_mean_std/kit','eval_std.npy'))
    
    #################################################################################
    #                                      Models                                   #
    #################################################################################
    model_dir = pjoin(args.checkpoints_dir, args.dataset_name, args.name, 'model')
    result_dir = pjoin('./generation', args.name + '_' + args.dataset_name + '_single_pose')
    os.makedirs(result_dir, exist_ok=True)

    ae = AE_models[args.ae_model](input_width=dim_pose)
    ckpt = torch.load(pjoin(args.checkpoints_dir, args.dataset_name, args.ae_name, 'model',
                            'latest.tar' if args.dataset_name == 't2m' else 'net_best_fid.tar'), map_location='cpu')
    model_key = 'ae'
    ae.load_state_dict(ckpt[model_key])
    if torch.cuda.is_available():
        ae=ae.cuda()

    ema_mardm = MARDM_models[args.model](ae_dim=ae.output_emb_width, cond_mode='text')
    model_dir = pjoin(model_dir, 'latest.tar')
    checkpoint = torch.load(model_dir, map_location='cpu')
    missing_keys2, unexpected_keys2 = ema_mardm.load_state_dict(checkpoint['ema_mardm'], strict=False)
    assert len(unexpected_keys2) == 0
    assert all([k.startswith('clip_model.') for k in missing_keys2])

    length_estimator = LengthEstimator(512, 50)
    ckpt = torch.load(pjoin(args.checkpoints_dir, args.dataset_name, 'length_estimator', 'model', 'finest.tar'),
                      map_location='cpu')
    length_estimator.load_state_dict(ckpt['estimator'])

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    ae = ae.to(device)
    ema_mardm = ema_mardm.to(device)
    length_estimator = length_estimator.to(device)

    ae.eval()
    ema_mardm.eval()
    length_estimator.eval()
    
    #################################################################################
    #                                     Sampling                                  #
    #################################################################################
    prompt_list = []
    
    if args.text_prompt != "":
        prompt_list.append(args.text_prompt)
    elif args.text_path != "":
        with open(args.text_path, 'r') as f:
            lines = f.readlines()
            for line in lines:
                line = line.strip()
                if line:
                    prompt_list.append(line)
    else:
        raise ValueError("éœ€è¦æä¾› --text_prompt æˆ– --text_path å‚æ•°ï¼")

    # ä½¿ç”¨å›ºå®šçš„çŸ­åºåˆ—é•¿åº¦ï¼ˆæœ€å°‘4å¸§ï¼‰
    # ç”Ÿæˆåæˆ‘ä»¬åªå–æŒ‡å®šå¸§
    token_lens = torch.LongTensor([args.sequence_length // 4] * len(prompt_list))
    token_lens = token_lens.to(device).long()
    m_length = token_lens * 4
    
    captions = prompt_list
    kinematic_chain = kit_kinematic_chain if args.dataset_name == 'kit' else t2m_kinematic_chain

    print(f"ğŸ“ æ–‡æœ¬æç¤ºè¯: {captions}")
    print(f"ğŸ¯ æå–å¸§ç´¢å¼•: {args.frame_index} (ä»ç”Ÿæˆçš„{args.sequence_length}å¸§åºåˆ—ä¸­)")
    print(f"ğŸ”„ é‡å¤æ¬¡æ•°: {args.repeat_times}")
    print(f"=" * 60)

    all_results = []

    for r in range(args.repeat_times):
        print(f"\n-->é‡å¤ {r+1}/{args.repeat_times}")
        with torch.no_grad():
            pred_latents = ema_mardm.generate(captions, token_lens, args.time_steps, args.cfg,
                                              temperature=args.temperature, hard_pseudo_reorder=args.hard_pseudo_reorder)
            pred_motions = ae.decode(pred_latents)
            pred_motions = pred_motions.detach().cpu().numpy()
            data = pred_motions * std + mean

        for k, (caption, joint_data) in enumerate(zip(captions, data)):
            s_path = pjoin(result_dir, str(k))
            os.makedirs(s_path, exist_ok=True)
            
            # æˆªå–åˆ°æŒ‡å®šé•¿åº¦
            joint_data = joint_data[:m_length[k]]
            
            # è½¬æ¢ä¸ºXYZåæ ‡
            joint_sequence = recover_from_ric(torch.from_numpy(joint_data).float(), nb_joints).numpy()
            
            # æå–æŒ‡å®šå¸§
            if args.frame_index >= len(joint_sequence):
                print(f"âš ï¸  è­¦å‘Š: å¸§ç´¢å¼• {args.frame_index} è¶…å‡ºèŒƒå›´ (åºåˆ—é•¿åº¦: {len(joint_sequence)})ï¼Œä½¿ç”¨æœ€åä¸€å¸§")
                frame_idx = len(joint_sequence) - 1
            else:
                frame_idx = args.frame_index
            
            single_pose = joint_sequence[frame_idx]  # (22, 3)
            
            print(f"  æ ·æœ¬ {k}: \"{caption}\"")
            print(f"  - å§¿æ€å½¢çŠ¶: {single_pose.shape}")
            print(f"  - æå–å¸§: {frame_idx}/{len(joint_sequence)}")
            
            # ä¿å­˜æ–‡ä»¶å
            base_name = f"caption:{caption[:30]}_sample{k}_repeat{r}_frame{frame_idx}"
            
            # ä¿å­˜ä¸ºNPYæ ¼å¼
            npy_path = pjoin(s_path, base_name + ".npy")
            np.save(npy_path, single_pose)
            print(f"  âœ… NPYæ–‡ä»¶: {npy_path}")
            
            # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆæ–¹ä¾¿é˜…è¯»ï¼‰
            if args.save_json:
                json_data = {
                    "caption": caption,
                    "frame_index": frame_idx,
                    "num_joints": nb_joints,
                    "joints": single_pose.tolist(),
                    "joint_names": [
                        "pelvis", "left_hip", "right_hip", "spine1", "left_knee", "right_knee",
                        "spine2", "left_ankle", "right_ankle", "spine3", "left_foot",
                        "right_foot", "neck", "left_collar", "right_collar", "head",
                        "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
                        "left_wrist", "right_wrist"
                    ] if nb_joints == 22 else None
                }
                json_path = pjoin(s_path, base_name + ".json")
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, indent=2, ensure_ascii=False)
                print(f"  âœ… JSONæ–‡ä»¶: {json_path}")
            
            # å¯è§†åŒ–å¹¶ä¿å­˜å›¾ç‰‡
            if args.save_image:
                img_path = pjoin(s_path, base_name + ".png")
                plot_single_pose_3d(single_pose, kinematic_chain, 
                                   title=f"{caption}\n(Frame {frame_idx})",
                                   save_path=img_path)
            
            # æ”¶é›†ç»“æœ
            all_results.append({
                'caption': caption,
                'frame_index': frame_idx,
                'pose': single_pose,
                'npy_path': npy_path
            })
            
            print()
    
    print("=" * 60)
    print(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(all_results)} ä¸ªå•å¸§å§¿æ€")
    print(f"ğŸ“ ä¿å­˜ç›®å½•: {result_dir}")
    print("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ä»æ–‡æœ¬ç”Ÿæˆå•å¸§3Då§¿æ€')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--name', type=str, default='MARDM_SiT_XL')
    parser.add_argument('--ae_name', type=str, default="AE")
    parser.add_argument('--ae_model', type=str, default='AE_Model')
    parser.add_argument('--model', type=str, default='MARDM-SiT-XL')
    parser.add_argument('--dataset_name', type=str, default='t2m', 
                       choices=['t2m', 'kit', 'eval_t2m', 'eval_kit'])
    parser.add_argument('--dataset_dir', type=str, default='./datasets')
    parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--seed", type=int, default=3407, 
                       help="éšæœºç§å­")
    parser.add_argument("--time_steps", default=18, type=int,
                       help="æ‰©æ•£æ­¥æ•°")
    parser.add_argument("--cfg", default=4.5, type=float,
                       help="Classifier-free guidanceå¼ºåº¦")
    parser.add_argument("--temperature", default=1, type=float,
                       help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument('--hard_pseudo_reorder', action="store_true",
                       help="ä½¿ç”¨ç¡¬æ€§ä¼ªé‡æ’åº")
    
    # è¾“å…¥å‚æ•°
    parser.add_argument('--text_prompt', default='', type=str,
                       help='å•ä¸ªæ–‡æœ¬æç¤ºè¯ï¼Œä¾‹å¦‚: "A person raising both hands"')
    parser.add_argument('--text_path', type=str, default="",
                       help='åŒ…å«å¤šä¸ªæ–‡æœ¬æç¤ºè¯çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
    
    # å§¿æ€æå–å‚æ•°
    parser.add_argument("--sequence_length", default=16, type=int,
                       help="ç”Ÿæˆçš„åºåˆ—é•¿åº¦ï¼ˆå¸§æ•°ï¼‰ï¼Œå¿…é¡»æ˜¯4çš„å€æ•°ã€‚ç”ŸæˆçŸ­åºåˆ—æ›´å¿«ã€‚")
    parser.add_argument("--frame_index", default=-1, type=int,
                       help="ä»ç”Ÿæˆåºåˆ—ä¸­æå–çš„å¸§ç´¢å¼•ã€‚-1è¡¨ç¤ºä¸­é—´å¸§ï¼Œ0è¡¨ç¤ºç¬¬ä¸€å¸§ï¼Œ-2è¡¨ç¤ºæœ€åä¸€å¸§")
    parser.add_argument("--repeat_times", default=3, type=int,
                       help="ä¸ºæ¯ä¸ªæç¤ºè¯ç”Ÿæˆå¤šå°‘ä¸ªä¸åŒçš„å§¿æ€")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--save_json', action='store_true',
                       help='æ˜¯å¦ä¿å­˜JSONæ ¼å¼ï¼ˆæ–¹ä¾¿æŸ¥çœ‹åæ ‡ï¼‰')
    parser.add_argument('--save_image', action='store_true',
                       help='æ˜¯å¦ä¿å­˜PNGå›¾ç‰‡ï¼ˆå¯è§†åŒ–ï¼‰')
    
    args = parser.parse_args()
    
    # å‚æ•°éªŒè¯å’Œå¤„ç†
    if args.sequence_length % 4 != 0:
        raise ValueError(f"sequence_length å¿…é¡»æ˜¯4çš„å€æ•°ï¼Œå½“å‰å€¼: {args.sequence_length}")
    
    if args.sequence_length < 4:
        raise ValueError(f"sequence_length å¿…é¡»è‡³å°‘ä¸º4ï¼Œå½“å‰å€¼: {args.sequence_length}")
    
    # å¤„ç†frame_index
    if args.frame_index == -1:
        # é»˜è®¤ä½¿ç”¨ä¸­é—´å¸§
        args.frame_index = args.sequence_length // 2
    elif args.frame_index == -2:
        # æœ€åä¸€å¸§
        args.frame_index = args.sequence_length - 1
    elif args.frame_index < 0:
        # è´Ÿç´¢å¼•
        args.frame_index = args.sequence_length + args.frame_index
    
    if args.frame_index < 0 or args.frame_index >= args.sequence_length:
        raise ValueError(f"frame_index {args.frame_index} è¶…å‡ºèŒƒå›´ [0, {args.sequence_length})")
    
    main(args)

